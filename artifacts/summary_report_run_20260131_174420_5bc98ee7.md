# Evaluation Summary Report (Canonical)

**Dataset**: public_updated.csv

## Overall Metrics

- **Total Tasks**: 50
- **Average Score**: 1.000 Â± 0.000
- **Score Range**: [1.000, 1.000]
- **Contradiction Rate**: 0.0%
- **Disagreement Rate**: 0.0%

## By Difficulty Level

| Difficulty | N | Avg Score | Std | Contradiction | Disagreement |
|------------|---|-----------|-----|---------------|--------------|
| Easy | 22 | 1.000 | 0.000 | 0.0% | 0.0% |
| Medium | 14 | 1.000 | 0.000 | 0.0% | 0.0% |
| Hard | 14 | 1.000 | 0.000 | 0.0% | 0.0% |

## By Question Type

| Type | N | Avg Score | Std |
|------|---|-----------|-----|
| Qualitative Retrieval | 9 | 1.000 | 0.000 |
| Quantitative Retrieval | 9 | 1.000 | 0.000 |
| Numerical Reasoning | 8 | 1.000 | 0.000 |
| Beat or Miss | 7 | 1.000 | 0.000 |
| Financial Modeling | 4 | 1.000 | 0.000 |
| Adjustments | 4 | 1.000 | 0.000 |
| Market Analysis | 3 | 1.000 | 0.000 |
| Trends | 3 | 1.000 | 0.000 |
| Complex Retrieval | 3 | 1.000 | 0.000 |

## Judge Performance

| Judge | Calls | Avg Latency (ms) | P95 Latency | Errors |
|-------|-------|------------------|-------------|--------|
| semantic_equivalence | 50 | 2615 | 5583 | 0 |
| numeric_tolerance | 50 | 2 | 4 | 0 |
| contradiction | 50 | 856 | 1394 | 0 |